{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Immigration and climate patterns in major US cites\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "## Project Summary\n",
    "In this project we combine demographic, immigration and climate data for the major cities of the United States. The data are first transformed with Spark and then loaded via an S3 bucket to an Amazon Redshift cluster that acts as a warehouse for analysts. The data will allow detailed analsys of residency and citizenship of visitors to various major cities in the United States, the most important airlines in terms of passenger numbers and the relationship between visitor numbers, local climate and the demographics of the US cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Install libraries\n",
    "# =============================================================================\n",
    "!pip uninstall -y numpy\n",
    "!pip install numpy==1.16.1\n",
    "!pip install fiona\n",
    "!pip install beautifulsoup4\n",
    "!pip install shapely\n",
    "!pip install pyspark\n",
    "!pip install pyarrow\n",
    "!pip install ipython-sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load libraries\n",
    "# =============================================================================\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from tempfile import NamedTemporaryFile\n",
    "from zipfile import ZipFile\n",
    "from shutil import rmtree, unpack_archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Scope of the project and data sources\n",
    "\n",
    "### Scope of project \n",
    "\n",
    "The project is designed to provide a mixed data set that will allow a team of data analysts/scientists to explore the relationship between climate, demographics and non-immigrant visitor patterns to various major US cities, including the origin of the visitors and the main airlines used for each city.\n",
    "\n",
    "### Description of the data sources\n",
    "\n",
    "The core data that we use comes from the three official sources. The first and largest one comes from an US government website giving a breakdown of international visitors to an individual visitor level at each port of entry with many interesting meta data relating to the visitor such as gender, type of visa and country of origin.  \n",
    "The second source is monthly temperature data for major cities around the world starting about 1750. This dataset is provided by Kaggle and originates from the Lawrence Berkeley National Laboratory.  \n",
    "The final and smallest dataset is hosted on opendatasoft.com and contains demographic information of US cities and census-designated places with a population greater or equal to 65,000. This data orignated from the US Census Bureau's 2015 American Community Survey.\n",
    "\n",
    "For the project we will mainly use these three data sets, augmented with some further data from public sources to help join the main data tables where this is possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Taking a look at the data\n",
    "\n",
    "At this stage we will not yet load the whole data where possible but just enough to do some exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "# =============================================================================\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"false\") \\\n",
    "    .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load data\n",
    "# =============================================================================\n",
    "demographics = spark \\\n",
    "    .read \\\n",
    "    .csv('us-cities-demographics.csv', sep=';', header='true')\n",
    "\n",
    "immigration_sample = spark \\\n",
    "    .read \\\n",
    "    .csv('immigration_data_sample.csv', sep=',', header='true')\n",
    "\n",
    "climate = spark \\\n",
    "    .read \\\n",
    "    .csv('../../data2/GlobalLandTemperaturesByCity.csv', header='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_sample.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "climate.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Exploration and Assement of the data\n",
    "\n",
    "Before going on we have a look at the data to find out if there are data quality issues \n",
    "and how easy or difficult it will be to connect the data through their columns and if \n",
    "we need to engineer any extra features.\n",
    "\n",
    "We shall start with the three main tables for immigration, climate and demographics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let us have a closer look at the demographics table first.\n",
    "demographics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics.describe().show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's count the number of records.\n",
    "demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# And now the number of cities represented in the dataset\n",
    "demographics.select(F.countDistinct('City', 'State')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's see some of the cities represented\n",
    "demographics.select('City').distinct().show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics.select('Race').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "So there are four times as many records as there are cities. We also noticed earlier that the dataframe has a column `Total Population` and columns `Race` and `Count`. So we could unstack the table by pivoting the `Race` and `Count` columns so that each race description forms its own column with the number of people in each racial group as the value. We should then end up with a dataframe that contains\n",
    "596 rows in 15 columns.\n",
    "\n",
    "I would also note that the dataset has a `State`, `State Code` and `City` column that might be good carry out SQL-joins in Redshift.\n",
    "\n",
    "In terms of cleaning, the data looks already good and probably does not need any further cleaning steps. Either the `State` or `State Code` column could possibly be removed as either column is sufficient for joins and a separate lookup table for states and state codes is already available and there are no missing values there. There are up to 16 missing values in the other columns. However, out of 2891 columns overall the percentage is very small and these missing values are not critical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let us move on to immigration and look first at the columns\n",
    "immigration_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_sample.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# And now some summary statistics\n",
    "immigration_sample.describe().show(vertical=True)\n",
    "\n",
    "# Clean up sample data. We won't need this anymore\n",
    "del immigration_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From the output above and the immigration meta-data file we can see that not all columns are useful. I would aggregate the data on port of entry and year and month of arrival. I would aggregate gender, country of origin, country of residency, length of stay (calculated from arrival and departure dates), age, visa type, and the airline used and drop the rest of the columns as many are mostly `null` anyway.\n",
    "Looking at the description data it is obvious that many columns need to be combined with the meta-data to be decoded . We will use the ports of entry, and country of origin information from the meta-data file for that. For airlines we will have to find a look-up table to convert the airline codes to names.\n",
    "\n",
    "We will also have to find a way of effectively storing the data on country of origin and country of residency as we won't be able to store them as separate columns in the dataframe (there would be more than 200 columns otherwise!).\n",
    "\n",
    "Since both the demographics and the immigration data are best organised by location (city and state code or state) it makes sense to consider that as their potential joining columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's move on to the climate data\n",
    "climate.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "climate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "climate.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The climate data is again organised by location and date. The timestamps seem to be monthly, starting from 1743 and ending in 2013. So there seems no overlap with the immigration data which is encompassing the whole of 2016. I would suggest to keep the last 50 years from 1963 to 2013 as this would be sufficient for an analysis of average temperatures and trends.  \n",
    "\n",
    "The locations include places inside and outside of the United States and I would drop rows with the latter. The `city` column does not include information on the US state or territory which could lead to mix-ups for city names that exist in more than one US state. However, we have information on latitude and longitude that we can use to engineer this feature.  \n",
    "\n",
    "There are some missing values and we will need to filter these out.  \n",
    "\n",
    "Again the climate data contains location data and monthly time points and that would be the best columns to carry out joins with the demographics and immigration data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Creating the meta-data\n",
    "\n",
    "To make effective use of the data above we have to find some extra information. Some of this is readily stored in the meta-data file for the immigration data set. However, we need some external lookup table to translate the airline codes to airline names and we need a shapefile to convert the latitude-longitude pairs in the climate data to US states and territories. So before we go on we gather these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Extract immigration meta data\n",
    "# =============================================================================\n",
    "\n",
    "! mkdir meta_data\n",
    "\n",
    "# Define functions \n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(\"^[ ]*'|[ ]*'[ ]*$\", \"\", text)\n",
    "    if re.sub(\"[ ]*\", \"\", cleaned_text).isnumeric():\n",
    "        return int(cleaned_text.strip())\n",
    "    else:\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "\n",
    "def convert_to_dictionary(large_string):\n",
    "    list_of_strings = [x for x in large_string.split('\\n')]\n",
    "    list_of_strings = [x for x in list_of_strings if x != '']\n",
    "    list_of_strings = [list(x.split(\"=\")) for x in list_of_strings]\n",
    "    return {ind: [clean_text(x[0]), clean_text(x[1])] for ind, x\n",
    "            in enumerate(list_of_strings)}\n",
    "\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    if len(list_of_lists) == 0:\n",
    "        return list_of_lists\n",
    "    if isinstance(list_of_lists[0], list):\n",
    "        return flatten(list_of_lists[0]) + flatten(list_of_lists[1:])\n",
    "    return list_of_lists[:1] + flatten(list_of_lists[1:])\n",
    "\n",
    "with open('I94_SAS_Labels_Descriptions.SAS', 'r') as conn:\n",
    "    description = conn.read()\n",
    "\n",
    "# Split text into list of elements\n",
    "description_list = description.split('\\n\\n')\n",
    "# Next we remove further elements that are not useful and split the text into\n",
    "# a list of strings.\n",
    "description_list = [x for x in description_list if re.search('[ ]?value ', x)]\n",
    "description_list = [re.sub(r'/\\*[^\\*]*\\*/[\\n]?', '', x)\n",
    "                    for x in description_list]\n",
    "description_list = [re.sub(r'[\\s]?value.*\\n', '', x) for x in description_list]\n",
    "description_list = [re.sub('[\\n| ];', '', x) for x in description_list]\n",
    "description_list = [re.sub('\\t', '', x) for x in description_list]\n",
    "\n",
    "# =============================================================================\n",
    "# Transform the individual tables and write to disk\n",
    "# =============================================================================\n",
    "\n",
    "# Carry out the split\n",
    "countries, _, _, us_states = description_list\n",
    "\n",
    "# Convert the different text elements to dataframes and clean them up\n",
    "# and finally write them to disk\n",
    "countries = convert_to_dictionary(countries)\n",
    "countries = pd.DataFrame.from_dict(countries, orient='index',\n",
    "                                columns=['country_code', 'country_name'])\n",
    "countries = countries[~countries['country_name'].str.contains(\n",
    "    \"INVALID|No Country Code|Collapsed\", case=False)]\n",
    "countries['country_name'] = countries['country_name'].str.title() \\\n",
    "    .str.replace(r'\\([^\\(\\)]*\\)', '') \\\n",
    "    .str.strip()\n",
    "\n",
    "countries.to_csv(os.path.join(os.getcwd(), 'meta_data/countries.csv'),\n",
    "              index=False)\n",
    "\n",
    "# countries = spark.createDataFrame(countries)\n",
    "countries = spark \\\n",
    "    .read \\\n",
    "    .csv('meta_data/countries.csv', sep=',', header='true')\n",
    "\n",
    "# Same again for the lookup-table for US states\n",
    "us_states = convert_to_dictionary(us_states)\n",
    "us_states = pd.DataFrame.from_dict(us_states, orient='index',\n",
    "                                   columns=['state_code', 'state'])\n",
    "\n",
    "us_states = us_states[(us_states['state_code'] != 99)]\n",
    "us_states['state'] = us_states['state'].str.replace(r'N\\.', 'NORTH') \\\n",
    "    .str.replace(r'S\\.', 'SOUTH') \\\n",
    "    .str.replace(r'W\\.', 'WEST') \\\n",
    "    .str.replace(r'DIST\\.', 'DISTRICT') \\\n",
    "    .str.title()\n",
    "us_states.to_csv('meta_data/us_states.csv', index=False)\n",
    "\n",
    "# us_states = spark.createDataFrame(us_states)\n",
    "us_states = spark \\\n",
    "    .read \\\n",
    "    .csv('meta_data/us_states.csv', sep=',', header='true')\n",
    "\n",
    "# Clean up\n",
    "del description, description_list, _, clean_text, convert_to_dictionary, flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The meta-data in the immigration meta-data file is not helpful to pin immigration ports to a certain city and this will need to be done by hand and then be ingested as a separate file. Doing this will also allow for the fact that some larger cities (notably New York City) have more than one international airport serving them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ports_of_entry = spark \\\n",
    "    .read \\\n",
    "    .csv('port_city_lookup.csv', sep=',', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Download lookup table of airlines codes and names\n",
    "# =============================================================================\n",
    "# import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define functions\n",
    "def extract_data(response):\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    table = soup.find(\"table\")\n",
    "    return pd.read_html(str(table))[0]\n",
    "\n",
    "# Load data\n",
    "airlines_url = 'https://www.bts.gov/topics/airlines-and-airports/airline-codes'\n",
    "\n",
    "response = requests.get(airlines_url)\n",
    "if response.status_code == 200:\n",
    "    airline_codes = extract_data(response)\n",
    "\n",
    "# Clean data\n",
    "airline_codes = airline_codes \\\n",
    "    .rename(str.lower, axis='columns') \\\n",
    "    .rename({'airline': 'airline_name'}, axis='columns') \\\n",
    "    .loc[lambda df: ~df['code'].str.contains(r'\\([0-9]\\)', na=False), ]\n",
    "\n",
    "# Write data to file\n",
    "airline_codes.to_csv('meta_data/airlines.csv', index=False)\n",
    "\n",
    "# Convert to PySpark dataframe\n",
    "# airline_codes = spark.createDataFrame(airline_codes)\n",
    "airline_codes = spark \\\n",
    "    .read \\\n",
    "    .csv('meta_data/airlines.csv', sep=',', header='true')\n",
    "\n",
    "# Clean up\n",
    "del airlines_url, response, extract_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Download shapefiles\n",
    "# =============================================================================\n",
    "# import libraries\n",
    "from urllib.request import urlopen\n",
    "import fiona\n",
    "from shapely.geometry import shape, Point, Polygon\n",
    "!mkdir shapefiles\n",
    "\n",
    "# Download shape file for climate data\n",
    "shapefile_url = 'https://www2.census.gov/geo/tiger/GENZ2019/shp/cb_2019_us_state_' \\\n",
    "       '500k.zip'\n",
    "with urlopen(shapefile_url) as zipresp, NamedTemporaryFile() as tfile:\n",
    "    tfile.write(zipresp.read())\n",
    "    tfile.seek(0)\n",
    "    unpack_archive(tfile.name, './shapefiles', format = 'zip')\n",
    "    \n",
    "del shapefile_url\n",
    "\n",
    "state_polygons = fiona.open('shapefiles/cb_2019_us_state_500k.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Let's briefly look at the meta-data\n",
    "\n",
    "#### Ports of entry lookup-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "ports_of_entry.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Countries of origin lookup-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### US state lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_states.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airlines lookup-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airline_codes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Geographic information on US states and territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='usa.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The shapefile with the state and territories of the United States shown above \n",
    "can be used to determine the state in which a geographic point given in latitude-longitude\n",
    "coordinates is located. Plotting this is currently not possible on the server as it \n",
    "requires larger system packages to be installed. So in this instance the image was\n",
    "rendered in QGIS and then imported.\n",
    "\n",
    "The meta-data looks all good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: The Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "The data will be stored in a relational database and we therefore need to model how the different tables can be joined with each other and if there are any foreign-key relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Structure of the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "Image(filename='capstone_project.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The final dataset will be made up of five tables:  \n",
    "The __city_demographics__ table contains the city demographics such as population numbers by gender and ethnicity for each major city in the US.  \n",
    "The __city_climate__ table contains the monthly average temperature for each city and is linked to the __city_demographics__ table by the _city_ and _state_ columns. However, they only partially overlap. \n",
    "\n",
    "The third table, __city_visitors__, contains the the monthly aggregated number of overseas visitors for each city and is connected to the __city_demographics__ table by the _city_ and _state_ columns and to the __city_climate__ table by _city_, _state_, _year_, and _month_ columns. But again these sets are not overlapping, as the climate dataset only goes to 2013 and the visitor dataset only contains the year 2016. Also the cities are only parially overlapping. However, assuming that this is a pipleine where both datasets will be available for each month, it makes sense to keep the dataset as a timeline rather than to take the temperature mean for each city. \n",
    "\n",
    "The __airlines__ table contains the full names of the airlines and the number of passengers per _city_+_state_+_year_+_month_ combination carried by an airlines. As it is derived from the same overseas visitors dataset these four columns directly overlap. \n",
    "\n",
    "Last but not least the __country_of_origin__ table contains the number of residents and citizen from each country visiting a city each month. Similar to the __airlines__ table it is derived from the overseas visitors dataset. \n",
    "\n",
    "As is easily discernable the common link between the five tables are the _city_ and _state_ columns. However, as the datasets did not contain all the same sets of cities there is only limited overlap. Four out of the five tables also have _year_ and _month_ columns but again only __city_visitors__, __airlines__ and __country_of_origin__ share the same time-range. However, there is still enough overlap of keys and information to gleen interesting insights, especially about the origin of visitors to the different cities, and which airlines are the biggest carriers to each city.\n",
    "\n",
    "All the tables will already be denormalised and there is no formal fact-table-dimension-table relationship. However, the table can be joined to form more elaborate queries.\n",
    "\n",
    "#### Treating the demographics table\n",
    "\n",
    "Exploring the cities dataset showed that the table was structed as a 'stacked' table. As such several rows referred to the same city but contained the different population groups as separate rows. To make the table more easy to deal with we will have to 'unstack' it by carrying out a pivot operation.\n",
    "\n",
    "### 3.2 The necessary data pipelines\n",
    "To process the data with maximum performance we will first carry out the transformations on the local machine before loading the resulting files to Redshift. As the data is over 6GB it would not be sensible to carry out the aggregation at a later stage.\n",
    "The aggregation will be carried out with PySpark with the aggregation columns being _state_, _city_, _year_, and _month_, _airline_code_ and _country_of_origin_ where they are available. This should reduce the size of the dataset considerably.\n",
    "We then create an S3 bucket and a Redshift Cluster. We create the empty tables in the Redshift cluster, load the data to the \n",
    "S3 bucket and finally copy the data from the S3 bucket to the Redshift cluster. \n",
    "\n",
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Creating the data model\n",
    "Now we run the code that will create the staging tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "!mkdir staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Load the full immigration dataset into Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_path = '../../data/18-83510-I94-Data-2016'\n",
    "immigration_files = os.listdir(immigration_path)\n",
    "immigration_files = [os.path.join(immigration_path, f)\n",
    "                     for f in immigration_files]\n",
    "\n",
    "def union_all(dfs):\n",
    "    if len(dfs) > 1:\n",
    "        return dfs[0].unionAll(union_all(dfs[1:]))\n",
    "    else:\n",
    "        return dfs[0]\n",
    "\n",
    "immigration = [spark.read.format('com.github.saurfang.sas.spark').load(f)\n",
    "               for f in immigration_files]\n",
    "\n",
    "keep_cols = ['i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate', 'i94mode',\n",
    "             'depdate', 'i94bir', 'i94visa', 'gender', 'airline', 'admnum', 'count']\n",
    "\n",
    "immigration = [df.select(*keep_cols) for df in immigration]\n",
    "immigration = union_all(immigration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Combine immigration data with the prepared meta data sets and write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Define functions\n",
    "# =============================================================================\n",
    "def cnt_cond(cond):\n",
    "    return F.sum(F.when(cond, 1).otherwise(0))\n",
    "\n",
    "# =============================================================================\n",
    "# Define data transformations in Spark\n",
    "# =============================================================================\n",
    "immigration = immigration \\\n",
    "    .dropna(how='any', subset='i94mode') \\\n",
    "    .filter(F.col('i94mode') ==  1) \\\n",
    "    .dropDuplicates(['admnum']) \\\n",
    "    .withColumn('year', F.col('i94yr').cast(IntegerType())) \\\n",
    "    .withColumn('month', F.col('i94mon').cast(IntegerType())) \\\n",
    "    .withColumn('stay', F.abs(F.col('depdate') - F.col('arrdate'))) \\\n",
    "    .withColumn('stay', F.col('stay').cast(IntegerType())) \\\n",
    "    .withColumn('i94cit', F.col('i94cit').cast(IntegerType())) \\\n",
    "    .withColumn('i94res', F.col('i94res').cast(IntegerType())) \\\n",
    "    .join(F.broadcast(ports_of_entry), F.col('i94port') == ports_of_entry.port_code,\n",
    "          how='inner') \\\n",
    "    .join(F.broadcast(us_states), on=['state_code'], how='inner') \\\n",
    "    .select('city', 'state', 'year', 'month', 'i94cit', 'i94res', 'i94bir',\n",
    "            'i94visa', 'count', 'gender', 'stay', 'airline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "aggregated_immigration = immigration \\\n",
    "    .groupBy(['year', 'month', 'city', 'state']) \\\n",
    "    .agg(F.count('count').alias('total_count'),\n",
    "         cnt_cond(F.col('gender') == \"M\").alias('male_count'),\n",
    "         cnt_cond(F.col('gender') == \"F\").alias('female_count'),\n",
    "         cnt_cond(F.col('i94visa') == 1).alias('business_visa_count'),\n",
    "         cnt_cond(F.col('i94visa') == 2).alias('tourist_visa_count'),\n",
    "         cnt_cond(F.col('i94visa') == 3).alias('student_visa_count'),\n",
    "         F.expr('percentile(i94bir, array(0.5))')[0].alias('median_visitor_age'),\n",
    "         F.expr('percentile(stay, array(0.5))')[0].alias('median_stay')) \\\n",
    "    .selectExpr('city', 'state', 'year', 'month', 'total_count', 'male_count',\n",
    "            'female_count', 'business_visa_count', 'tourist_visa_count',\n",
    "            'student_visa_count', \n",
    "            'round(median_visitor_age, 1) median_visitor_age',\n",
    "            'round(median_stay, 2) median_stay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airlines = immigration \\\n",
    "    .na.drop(subset=['airline']) \\\n",
    "    .withColumn('airline_code', F.regexp_replace(F.col('airline'), \"\\\\*\", \"\")) \\\n",
    "    .groupBy(['city', 'state', 'year', 'month', 'airline_code']) \\\n",
    "    .agg(F.count('count').alias('passenger_number')) \\\n",
    "    .join(F.broadcast(airline_codes), F.col('airline_code') == airline_codes.code, how='left') \\\n",
    "    .select('city', 'state', 'year', 'month', 'airline_code',\n",
    "            'airline_name', 'passenger_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "citizenship = immigration \\\n",
    "    .groupBy(['city', 'state', 'year', 'month', 'i94cit']) \\\n",
    "    .agg(F.count('count').alias('citizens_visiting')) \\\n",
    "    .join(F.broadcast(countries), F.col('i94cit') == countries.country_code, how='inner') \\\n",
    "    .select('city', 'state', 'year', 'month', 'country_name',\n",
    "            'citizens_visiting')\n",
    "\n",
    "residency = immigration \\\n",
    "    .groupBy(['city', 'state', 'year', 'month', 'i94res']) \\\n",
    "    .agg(F.count('count').alias('residents_visiting')) \\\n",
    "    .join(F.broadcast(countries), F.col('i94res') == countries.country_code, how='inner') \\\n",
    "    .select('city', 'state', 'year', 'month', 'country_name',\n",
    "            'residents_visiting')\n",
    "\n",
    "visitors_by_country = citizenship \\\n",
    "    .join(residency,  on=['year', 'month', 'city', 'state', 'country_name'],\n",
    "          how='outer') \\\n",
    "    .na.drop(subset=['country_name']) \\\n",
    "    .fillna(0, subset=['citizens_visiting', 'residents_visiting']) \\\n",
    "    .select('city', 'state', 'year', 'month', 'country_name',\n",
    "            'citizens_visiting', 'residents_visiting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Write Spark dataframes to disk\n",
    "# =============================================================================\n",
    "aggregated_immigration \\\n",
    "    .repartition('state') \\\n",
    "    .write \\\n",
    "    .csv('staging/immigration/',\n",
    "         mode='overwrite')\n",
    "\n",
    "del aggregated_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airlines \\\n",
    "    .repartition('state') \\\n",
    "    .write \\\n",
    "    .csv('staging/airlines/',\n",
    "         mode='overwrite')\n",
    "\n",
    "del airlines, airline_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visitors_by_country \\\n",
    "    .repartition('state') \\\n",
    "    .write \\\n",
    "    .csv('staging/countries/',\n",
    "         mode='overwrite')\n",
    "\n",
    "del residency, citizenship, visitors_by_country, immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Transform and write demographic data to staging folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Sort out column names\n",
    "# =============================================================================\n",
    "# Column names in the demogrpahic data table is upper case with spaces and\n",
    "# hyphens as word separators. We have to clean this before we can continue.\n",
    "demographics_columns = demographics.columns\n",
    "new_names = [re.sub(\"[ -]\", \"_\", col).lower() for col in demographics_columns]\n",
    "\n",
    "demographics = demographics.toDF(*new_names)\n",
    "\n",
    "# =============================================================================\n",
    "# Pivot and transform table\n",
    "# =============================================================================\n",
    "column_match = [re.match(\"race|count\", city) is None\n",
    "                for city in demographics.columns]\n",
    "\n",
    "index_cols = [name for name, match in\n",
    "              zip(demographics.columns, column_match) if match]\n",
    "\n",
    "demographics = demographics.groupBy(index_cols) \\\n",
    "    .pivot(\"race\") \\\n",
    "    .agg(F.sum('count')) \\\n",
    "    .drop('state_code') \\\n",
    "    .withColumnRenamed('median_age', 'median_population_age') \\\n",
    "    .dropna(how='any', subset=('state', 'city')) \\\n",
    "    .fillna(0) \\\n",
    "    .selectExpr('city', 'state',\n",
    "                'cast(male_population as int) male_population',\n",
    "                'cast(female_population as int) female_population',\n",
    "                'cast(total_population as int) total_population',\n",
    "                'cast(number_of_veterans as int) veterans',\n",
    "                'cast(foreign_born as int) foreign_born',\n",
    "                'cast(`American Indian and Alaska Native` as int) '\n",
    "                'native_american',\n",
    "                'cast(Asian as int) asian',\n",
    "                'cast(`Black or African-American` as int) black',\n",
    "                'cast(`Hispanic or Latino` as int) latino',\n",
    "                'cast(White as int) white',\n",
    "                'median_population_age', 'average_household_size',)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Write Spark dataframe to disk\n",
    "# =============================================================================  \n",
    "demographics \\\n",
    "    .repartition('state') \\\n",
    "    .write \\\n",
    "    .csv('staging/demographics/',\n",
    "         mode='overwrite')\n",
    "\n",
    "del  demographics_columns, demographics, index_cols, new_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Transform and stage climate data\n",
    "\n",
    "The climate data needs the most work. The data is first aggregated before its combined with the shapefile data to extract the state from its latitude and longitude columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load shape file\n",
    "# =============================================================================\n",
    "state_polygons = fiona.open('shapefiles/cb_2019_us_state_500k.shp')\n",
    "\n",
    "# =============================================================================\n",
    "# Initial transformation of climate data\n",
    "# =============================================================================\n",
    "climate = climate \\\n",
    "    .toDF(*[c.lower() for c in climate.columns]) \\\n",
    "    .dropna(how='any') \\\n",
    "    .filter(F.col('country') == \"United States\") \\\n",
    "    .withColumn('dt', F.col('dt').cast(DateType())) \\\n",
    "    .withColumn('year', F.year(F.col('dt'))) \\\n",
    "    .withColumn('month', F.month(F.col('dt'))) \\\n",
    "    .filter(F.col('year') >= 1963)\n",
    "\n",
    "city_coordinates = climate \\\n",
    "    .select('latitude', 'longitude') \\\n",
    "    .dropDuplicates() \\\n",
    "    .toPandas()\n",
    "\n",
    "# =============================================================================\n",
    "# Define transformation functions\n",
    "# =============================================================================\n",
    "def _convert_direction_to_plus_minus(coordinate):\n",
    "    numeric_part = float(coordinate[:-1])\n",
    "    direction = coordinate[-1].lower()\n",
    "    converter = {\"e\": +1,\n",
    "                 \"n\": +1,\n",
    "                 \"s\": -1,\n",
    "                 \"w\": -1}\n",
    "    return numeric_part * converter[direction]\n",
    "\n",
    "\n",
    "def _get_state_name_for_coordinates(features, coordinates):\n",
    "    point = Point(coordinates[::-1])\n",
    "    feature_dict = {feat['properties']['STUSPS']: shape(feat['geometry'])\n",
    "                    for feat in features}\n",
    "    for k, v in feature_dict.items():\n",
    "        if v.contains(point):\n",
    "            return k\n",
    "    for k, v in feature_dict.items():\n",
    "        if v == min(feature_dict.values(), key=point.distance):\n",
    "            return k\n",
    "\n",
    "\n",
    "def convert_coordinates_to_states(features, coordinates):\n",
    "    coordinates = tuple(_convert_direction_to_plus_minus(item) for item in\n",
    "                        coordinates)\n",
    "    return _get_state_name_for_coordinates(features, coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Retreive US state where city is located\n",
    "# =============================================================================\n",
    "states = [convert_coordinates_to_states(state_polygons, row) for row in\n",
    "          zip(city_coordinates.loc[:, 'latitude'], \n",
    "              city_coordinates.loc[:, 'longitude'])]\n",
    "\n",
    "city_coordinates['state_code'] = states\n",
    "\n",
    "city_coordinates = spark.createDataFrame(city_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Combine climate data with state names\n",
    "# =============================================================================\n",
    "climate = climate \\\n",
    "    .join(city_coordinates, on=['latitude', 'longitude'], how='left') \\\n",
    "    .join(F.broadcast(us_states), on=['state_code'], how='inner') \\\n",
    "    .selectExpr('city', 'state', 'year', 'month',\n",
    "                'round(averagetemperature, 2) average_temperature',\n",
    "                'round(averagetemperatureuncertainty, 2)'\n",
    "                'temperature_uncertainty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Write Spark dataframe to disk\n",
    "# =============================================================================\n",
    "climate \\\n",
    "    .repartition('state') \\\n",
    "    .write \\\n",
    "    .csv('staging/climate/',\n",
    "         mode='overwrite')\n",
    "\n",
    "del city_coordinates, climate, states, state_polygons, us_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Check the aggregated data\n",
    "\n",
    "Before uploading the data first to S3 and then to the Redshift cluster we will check that all of the data is in the right format and that there are no unexpected irregularities. So we will load the now much smaller aggregated tables back in an look for irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_immigration  = spark \\\n",
    "    .read \\\n",
    "    .csv('staging/immigration/')\n",
    "\n",
    "test_airlines  = spark \\\n",
    "    .read \\\n",
    "    .csv('staging/airlines/')\n",
    "\n",
    "test_countries  = spark \\\n",
    "    .read \\\n",
    "    .csv('staging/countries/')\n",
    "\n",
    "test_demographics  = spark \\\n",
    "    .read \\\n",
    "    .csv('staging/demographics/')\n",
    "\n",
    "test_climate  = spark \\\n",
    "    .read \\\n",
    "    .csv('staging/climate/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As the csv files have no header we do not expect to find any meaningful column names. For each table we get the first five rows, count the number of rows in the table and look for the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_immigration.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_immigration.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in test_immigration.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_airlines.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_airlines.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in test_airlines.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_countries.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_countries.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_countries.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in test_countries.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_demographics.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_demographics.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in test_demographics.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_climate.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_climate.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_climate.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in test_climate.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see the columns that contain the primary keys do not contain missing values. That's great. It means we can move on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Setting up AWS infrastructure\n",
    "\n",
    "We will build the AWS from code directly in the pipeline. Any sensitive information is not stored inside the code but instead in a configuration file (`capstone.cfg`) that we parse into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import psycopg2\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('capstone.cfg')\n",
    "\n",
    "\n",
    "ACCESS_KEY             = config.get('AWS','ACCESS_KEY')\n",
    "SECRET_KEY             = config.get('AWS','SECRET_KEY')\n",
    "REGION                 = config.get('S3', 'REGION')\n",
    "BUCKET_NAME            = config.get('S3', 'BUCKET_NAME')\n",
    "\n",
    "DWH_CLUSTER_TYPE       = config.get(\"DWH\",\"DWH_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"DWH\",\"DWH_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"DWH\",\"DWH_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"DWH\",\"DWH_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"DWH\",\"DWH_DB\")\n",
    "DWH_DB_USER            = config.get(\"DWH\",\"DWH_DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"DWH\",\"DWH_DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"DWH\",\"DWH_PORT\")\n",
    "\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Amazon resource connections\n",
    "# =============================================================================\n",
    "s3 = boto3.resource('s3',\n",
    "                    region_name=REGION,\n",
    "                    aws_access_key_id=ACCESS_KEY,\n",
    "                    aws_secret_access_key=SECRET_KEY)\n",
    "iam = boto3.client('iam',\n",
    "                   region_name=REGION,\n",
    "                   aws_access_key_id=ACCESS_KEY,\n",
    "                   aws_secret_access_key=SECRET_KEY)\n",
    "\n",
    "redshift = boto3.client('redshift',\n",
    "                   region_name=REGION,\n",
    "                   aws_access_key_id=ACCESS_KEY,\n",
    "                   aws_secret_access_key=SECRET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Upload data to S3 staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create S3 bucket\n",
    "# =============================================================================\n",
    "s3.create_bucket(Bucket=BUCKET_NAME,\n",
    "                 CreateBucketConfiguration={'LocationConstraint': REGION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Define function to upload all files in a folder to S3\n",
    "# =============================================================================\n",
    "def upload_folders(directory_path, s3, bucket_name, prefix):\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                if re.search(r'SUCCESS|\\.crc$', file) is None:\n",
    "                    local_file = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(local_file, directory_path)\n",
    "                    s3_file = os.path.join(prefix, relative_path)\n",
    "                    s3.meta.client.upload_file(local_file, bucket_name, s3_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Upload files to S3\n",
    "# =============================================================================\n",
    "upload_folders('staging/immigration/', s3, BUCKET_NAME, 'staging/immigration/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "upload_folders('staging/airlines', s3, BUCKET_NAME, 'staging/airlines/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "upload_folders('staging/climate/', s3, BUCKET_NAME, 'staging/climate/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "upload_folders('staging/countries/', s3, BUCKET_NAME, 'staging/countries/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "upload_folders('staging/demographics', s3, BUCKET_NAME, 'staging/demographics/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean up the local staging folder\n",
    "!rm -r staging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Copy staging tables into Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create IAM role\n",
    "# =============================================================================\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Redshift cluster\n",
    "# =============================================================================\n",
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The next block will run until `ClusterStatus` changes to 'available' before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)\n",
    "\n",
    "while myClusterProps['ClusterStatus'] != 'available':\n",
    "    time.sleep(30)\n",
    "    prettyRedshiftProps(myClusterProps)\n",
    "    myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "\n",
    "prettyRedshiftProps(myClusterProps)\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "conn_string=f'postgresql://{DWH_DB_USER}:{DWH_DB_PASSWORD}@{DWH_ENDPOINT}:{DWH_PORT}/{DWH_DB}'\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Create Redshift tables\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS city_visitors;\n",
    "CREATE TABLE IF NOT EXISTS city_visitors (\n",
    "    city VARCHAR(100) NOT NULL,\n",
    "    state VARCHAR(30) NOT NULL,\n",
    "    year SMALLINT NOT NULL,\n",
    "    month  SMALLINT NOT NULL,\n",
    "    total_count INTEGER,\n",
    "    male_count INTEGER,\n",
    "    female_count INTEGER,\n",
    "    business_visa_count INTEGER,\n",
    "    tourist_visa_count INTEGER,\n",
    "    student_visa_count INTEGER,\n",
    "    median_visitor_age REAL,\n",
    "    median_stay REAL,\n",
    "    CONSTRAINT city_pkey PRIMARY KEY (city, state, year, month))\n",
    "    DISTSTYLE KEY\n",
    "    DISTKEY (state)\n",
    "    COMPOUND SORTKEY (state, city, year, month);\n",
    "\n",
    "    \n",
    "DROP TABLE IF EXISTS airline;\n",
    "CREATE TABLE IF NOT EXISTS airline (\n",
    "    city VARCHAR(100) NOT NULL,\n",
    "    state VARCHAR(30) NOT NULL,\n",
    "    year SMALLINT NOT NULL,\n",
    "    month  SMALLINT NOT NULL,\n",
    "    airline_code VARCHAR(3) NOT NULL,\n",
    "    airline_name VARCHAR(256),\n",
    "    passenger_number INTEGER,\n",
    "    CONSTRAINT airline_pkey PRIMARY KEY (city, state, year, month, airline_code))\n",
    "    DISTSTYLE KEY\n",
    "    DISTKEY (state)\n",
    "    COMPOUND SORTKEY (state, city, year, month, airline_code);\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS country_of_origin;\n",
    "CREATE TABLE IF NOT EXISTS country_of_origin (\n",
    "    city VARCHAR(100) NOT NULL,\n",
    "    state VARCHAR(30) NOT NULL,\n",
    "    year SMALLINT NOT NULL,\n",
    "    month  SMALLINT NOT NULL,\n",
    "    country_name VARCHAR(35) NOT NULL,\n",
    "    citizens_visiting INTEGER,\n",
    "    residents_visiting INTEGER,\n",
    "    CONSTRAINT country_pkey PRIMARY KEY (city, state, year, month, country_name))\n",
    "    DISTSTYLE KEY\n",
    "    DISTKEY (state)\n",
    "    COMPOUND SORTKEY (state, city, year, month, country_name);\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS city_demographics;\n",
    "CREATE TABLE IF NOT EXISTS city_demographics (\n",
    "    city VARCHAR(100) NOT NULL,\n",
    "    state VARCHAR(30) NOT NULL,\n",
    "    total_population INTEGER,\n",
    "    male_population INTEGER,\n",
    "    female_population INTEGER,\n",
    "    veterans INTEGER,\n",
    "    foreign_born INTEGER,\n",
    "    asian INTEGER,\n",
    "    black INTEGER,\n",
    "    latino INTEGER,\n",
    "    native_american INTEGER,\n",
    "    white INTEGER,\n",
    "    median_population_age REAL,\n",
    "    average_houshold_size REAL,\n",
    "    CONSTRAINT demographics_pkey PRIMARY KEY (city, state))\n",
    "    DISTSTYLE ALL\n",
    "    COMPOUND SORTKEY (state, city);\n",
    "\n",
    "\n",
    "DROP TABLE IF EXISTS city_climate;\n",
    "CREATE TABLE IF NOT EXISTS city_climate (\n",
    "    city VARCHAR(100) NOT NULL,\n",
    "    state VARCHAR(30) NOT NULL,\n",
    "    year SMALLINT NOT NULL,\n",
    "    month  SMALLINT NOT NULL,\n",
    "    average_temperature REAL,\n",
    "    temperature_uncertainty REAL,\n",
    "    CONSTRAINT climate_pkey PRIMARY KEY (city, state, year, month))\n",
    "    DISTSTYLE ALL\n",
    "    COMPOUND SORTKEY (state, city, year, month);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load staging tables to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's define a function to make this more convenient\n",
    "def copy_staging_table(target_table, bucket, prefix, iam_arn):\n",
    "    full_source_path = os.path.join(bucket, prefix)\n",
    "    return f\"\"\"COPY {target_table}\n",
    "            FROM \\'{full_source_path}\\'\n",
    "            IAM_ROLE \\'{iam_arn}\\'\n",
    "            FORMAT AS CSV;\"\"\"\n",
    "\n",
    "\n",
    "bucket_url = f's3://{BUCKET_NAME}/'\n",
    "\n",
    "# Now we create the sql-command strings that we will feed to Redshift\n",
    "copy_immigration = copy_staging_table('city_visitors',\n",
    "                                         bucket_url,\n",
    "                                         'staging/immigration/',\n",
    "                                         DWH_ROLE_ARN)\n",
    "\n",
    "copy_climate = copy_staging_table('city_climate',\n",
    "                                  bucket_url,\n",
    "                                  'staging/climate/',\n",
    "                                  DWH_ROLE_ARN)\n",
    "\n",
    "copy_demographics = copy_staging_table('city_demographics',\n",
    "                                       bucket_url,\n",
    "                                       'staging/demographics/',\n",
    "                                       DWH_ROLE_ARN)\n",
    "\n",
    "copy_airlines = copy_staging_table('airline',\n",
    "                                   bucket_url,\n",
    "                                   'staging/airlines/',\n",
    "                                   DWH_ROLE_ARN)\n",
    "\n",
    "copy_countries = copy_staging_table('country_of_origin',\n",
    "                                    bucket_url,\n",
    "                                    'staging/countries/',\n",
    "                                    DWH_ROLE_ARN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Send the copy-commands to Redshift for execution\n",
    "# =============================================================================\n",
    "%sql $copy_immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql $copy_climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql $copy_demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql $copy_airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql $copy_countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### To clean up we delete the folder containing the staging data on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# First we create a bucket instance\n",
    "staging_bucket = s3.Bucket(BUCKET_NAME, )\n",
    "\n",
    "# Then we can use the instance to delete folders\n",
    "_ = staging_bucket.objects.filter(Prefix='staging').delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "We shall briefly run a last set of checks on the database tables.  \n",
    "We start by checking that all keys are unique by checking that the number of rows equals the number of unique compound primary key values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "primary_keys = {'city_visitors': 'city, state, year, month',\n",
    "                'airline':  'city, state, year, month, airline_code',\n",
    "                'country_of_origin': 'city, state, year, month, country_name',\n",
    "                'city_climate': 'city, state, year, month',\n",
    "                'city_demographics' : 'city, state'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "primary_key_checks = {}\n",
    "for k, v in  primary_keys.items():\n",
    "    all_rows = %sql SELECT COUNT(*) FROM {k};\n",
    "    unique_rows = %sql SELECT COUNT(*) FROM (SELECT DISTINCT {v} FROM {k});\n",
    "    if all_rows==unique_rows:\n",
    "        print('==========================================================\\n'\n",
    "              f'All primary keys in table {k} are unique!!!\\n'\n",
    "              f'There are {all_rows[0][0]} records in this table.\\n'\n",
    "              '==========================================================\\n')\n",
    "    else:\n",
    "         print('==========================================================\\n'\n",
    "              f'ERROR!!. Primary keys in table {k} are NOT unique!!!\\n'\n",
    "              f'There are {all_rows[0][0]} records in this table.\\n'\n",
    "              f'{unique_rows[0][0]} records are unique.\\n'\n",
    "              '==========================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "So we can see that all primary keys are unique.  \n",
    "Finally we have a brief look at the tables to ensure that the data is not garbled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM city_visitors LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM airline ORDER BY passenger_number DESC LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM country_of_origin ORDER BY residents_visiting DESC LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM city_climate LIMIT 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM city_demographics LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "That looks all good as well. That means the pipeline is working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary \n",
    "\n",
    "__city_visitors__ table\n",
    "\n",
    "Table describing the monthly statistics for various overseas non-immigrant visitors arriving by air traffic arrving in certain US cities.\n",
    "\n",
    "| column name        | description                                   |\n",
    "| :----------------- | :-------------------------------------------- |\n",
    "| city               | Name of US city                               |\n",
    "| state              | Name of US state                              |\n",
    "| year               | Year as 4 digit integer                       |\n",
    "| month              | Month as 2 digit integer                      |\n",
    "| total_count        | Total number of visitors                      |\n",
    "| male_count         | Number of male visitors                       |\n",
    "| female_count       | Number of female visitors                     |\n",
    "|business_visa_count | Number of visitors on a business visa         |\n",
    "|tourist_visa_count  | Number of visitors on a tourist visa          |\n",
    "|student_visa_count  | Number of visitors on a student visa          |\n",
    "|median_visitor_age  | Median age of all visitors in years           |\n",
    "|median_stay         | Median length of stay of all visitors in days |\n",
    "\n",
    "\n",
    "__airline__ table\n",
    "\n",
    "Table describing the airlines and passenger numbers transported by each airline for certain US city per calendar month.\n",
    "\n",
    "| column name        | description                                             |\n",
    "| :----------------- | :------------------------------------------------------ |\n",
    "| city               | Name of US city                                         |\n",
    "| state              | Name of US state                                        |\n",
    "| year               | Year as 4 digit integer                                 |\n",
    "| month              | Month as 2 digit integer                                |\n",
    "| airline_code       | 2 or 3 alpha- numeric code given to each airline by ICE |\n",
    "| airline_name       | Full name of airline when available                     |\n",
    "| passenger_number   | Number of passengers arriving in a city by this airline |\n",
    "\n",
    "\n",
    "__country_of_origin__ table\n",
    "\n",
    "Table describing statistics on the citizenship and current country of residence of non-immigrant international visitors to a certain US city per calendar month.\n",
    "\n",
    "| column name        | description                                                         |\n",
    "| :----------------- | :------------------------------------------------------------------ |\n",
    "| city               | Name of US city                                                     |\n",
    "| state              | Name of US state                                                    |\n",
    "| year               | Year as 4 digit integer                                             |\n",
    "| month              | Month as 2 digit integer                                            |\n",
    "| country_name       | Full name of country                                                |\n",
    "| citizens_visiting  | Number of citizen from this particular country visiting the US city |\n",
    "| residents_visiting | Number of residents of this particular country visiting the US city |\n",
    "\n",
    "\n",
    "__city_demographics__ table\n",
    "\n",
    "Table describing the total population size, gender and racial make-up of US cities including social statistics.\n",
    "\n",
    "| column name            | description                                          |\n",
    "| :--------------------- | :--------------------------------------------------- |\n",
    "| city                   | Name of US city                                      |\n",
    "| state                  | Name of US state                                     |\n",
    "| total_population       | Total number of city inhabitants living in the city  |\n",
    "| male_population        | Number of male inhabitants living in the city        |\n",
    "| female_population      | Number of female inhabitants living in the city      |\n",
    "| veterans               | Number of veterans living in the city                |\n",
    "| foreign_born           | Number of foreign born people living in the city     |\n",
    "| asian                  | Number of people of Asian origin living in the city  |\n",
    "| black                  | Number of black people living in the city            |\n",
    "| latino                 | Number of people of Latino origin living in the city |\n",
    "| native_american        | Number of native american people living in the city  |\n",
    "| white                  | Number of white people living in the city            |\n",
    "| median_population_age  | Median age of city inhabintants in years             |\n",
    "| average_household_size | Average number of people per household               |  \n",
    "\n",
    "\n",
    "__city_climate__ table\n",
    "\n",
    "Table describing historic temperature trends for certain US cities.\n",
    "\n",
    "| column name             | description                                          |\n",
    "| :---------------------- | :--------------------------------------------------- |\n",
    "| city                    | Name of US city                                      |\n",
    "| state                   | Name of US state                                     |\n",
    "| year                    | Year as 4 digit integer                              |\n",
    "| month                   | Month as 2 digit integer                             |\n",
    "| average_temperature     | Average monthly temperature in degree Celsius        |\n",
    "| temperature_uncertainty | Uncertainty of monthly temperature in degree Celsius |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 5 Final thoughts on the project\n",
    "\n",
    "### 5.1 Choice of tools and technologies\n",
    "\n",
    "The reason we used PySpark was that we needed to aggregate a very large amount of data before being able to send it to the Redshift database.\n",
    "Pandas would not be able to handle a dataset that is 6GB in size.  \n",
    "Uploading to S3 was necessary as it is the easiest way to stage the aggregated data for copying to Redshift.  \n",
    "Finally, Redshift, is a good choice when hosting a relational database in the cloud, as it allows for later resizing when the volume of data or queries increases without having to migrate the data to a new database. \n",
    "\n",
    "Since the climate and immigration data is naturally aggregated every month it makes sense to run the pipeline on a monthly basis. However, this should be done with some lag time as the median stay has to be calculated from the departure date which will be some time after the arrival date.\n",
    "\n",
    "### 5.2 Possible changes in the technological approach to future challenges\n",
    "\n",
    "Whilst the purpose of this project is a case study there are various ways to adjust this pipeline for future challenges. \n",
    "If the input of data was suddenly increase in volume by, say 100 times, it would make most sense to host the PySpark step on an Amazon EMR cluster that can be appropriately sized to fit the increased workload. This would most likely also increase the speed of transfer of the parquet files to the S3 staging bucket. The S3 bucket and the Redshift cluster itself will not need resizing as the aggregated data will still be very small after processing with Spark.\n",
    "In case that the pipeline has to be run everyday in the early morning, at for example 7am, it would make more sense to build out the pipeline in Airflow that makes it easier to automate the process.  \n",
    "In case of increased user access to the database (say by 100 times) the Redshift cluster needs to be increased in size to cope with the additional queries. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
